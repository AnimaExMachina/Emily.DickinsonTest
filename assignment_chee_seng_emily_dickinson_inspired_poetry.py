# -*- coding: utf-8 -*-
"""Assignment Chee Seng - Emily Dickinson-inspired Poetry

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19o5WN0CDX-L69a-ovVi0D3ta65jRAaA2

# Introduction
When I first suggested this project during the consultation session after class, I had thought of using the GPT-2-Master framework. 

However, after additional research, I realised that using it would not be feasible on Colab (as it would time out before I could finish training the model). Hence, I turned to Max Woolf's [GPT-2-Simple](https://https://minimaxir.com/2019/09/howto-gpt2/) as the main model.

# Mounting Google Drive
"""

from google.colab import drive

drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/Deep Learning Project Alpha - Emily Dickinson/gpt-2-master/'

"""# Installing GPT-2-Simple and related packages"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
!pip install -q gpt-2-simple
import gpt_2_simple as gpt2
from datetime import datetime
from google.colab import files

from tensorflow.keras.callbacks import LambdaCallback

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.layers import LSTM
from tensorflow.keras.optimizers import RMSprop
from keras.utils.data_utils import get_file

import tensorflow as tf
import numpy as np
import random
import sys
import io

pip install fire

pip install regex

pip install requests

pip install tqdm

print(tf.keras.__version__)
print(tf.__version__)

!ls

"""# Loading Dataset"""

gpt2.mount_gdrive()
drive.mount("/content/drive")

gpt2.copy_file_from_gdrive('emily_dickinson_titleless.txt')

"""The data set loaded above was taken from [JenLooper's Kraggle data set](https://www.kaggle.com/jenlooper/emily-dickinson-poetry). In my earlier test iterations, I had tried to incorporate an API between my Colab Notebook and Kraggle. However, I realised it failed quite often while I was running it. So I extracted out the .txt file and placed it within my Google Drive instead."""

with io.open('emily_dickinson_titleless.txt', encoding='utf-8') as f:
    text = f.read().lower()
print('corpus length:', len(text))

text[:100]

len(text[:50])

!ls

chars = sorted(list(set(text)))
print('total chars:', len(chars))

print(chars)

"""# Downloading GPT-2-Simple Model """

gpt2.download_gpt2(model_name="355M")

"""# First Fine-Tuning Session for GPT-2-Simple Model"""

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset='/content/emily_dickinson_titleless.txt',
              model_name='355M',
              steps=300,
              restore_from='fresh',
              run_name='run1',
              print_every=10,
              sample_every=100,
              save_every=100
              )

gpt2.copy_checkpoint_from_gdrive(run_name='run1')

"""# Generating samples from the first fine-tuned model"""

import tensorflow as tf
tf.reset_default_graph()
sess = gpt2.start_tf_sess()
gpt2.load_gpt2(sess, run_name='run1')

gpt2.generate(sess,
              length=250,
              temperature=0.7,
              nsamples=10,
              batch_size=10,
              top_p=0.9,
              top_k=40
              )

"""As seen from above the generated samples from the first fine-tuned model doesn't seem to be very good. Perhaps by decreasing the loss further, we might be able to get a better result.

# Second Round of Fine-Tuning for GPT-2-Simple Model
"""

import tensorflow as tf
tf.reset_default_graph()
sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset='/content/emily_dickinson_titleless.txt',
              model_name='355M',
              steps=120,
              restore_from='latest',
              run_name='run1',
              print_every=10,
              sample_every=30,
              save_every=30
              )

gpt2.copy_checkpoint_from_gdrive(run_name='run1')

"""# Generating Samples from the second round of fine-tuning"""

import tensorflow as tf
tf.reset_default_graph()
sess = gpt2.start_tf_sess()
gpt2.load_gpt2(sess, run_name='run1')

gpt2.generate(sess,
              length=250,
              temperature=0.6,
              nsamples=10,
              batch_size=10,
              top_p=0.9,
              top_k=40
              )

"""# Conclusion & Further Research

After having run the model through two rounds of fine-tuning, we managed to reach a final loss score of 0.20, with the lowest loss score of 0.04.

Mathmatically, this loss score is fairly decent - given that we're supposed to minimise loss as much as possible. However, given that this is an attempt to generate poetry, our project metrics should not only factor the minimisation of loss as priority. Rather, it should also include factors such as 'legibility' (i.e. does the poem/stanza make meaningful sense when read by a human) as well as 'poetic epiphany' (i.e. does the poem/stanza strike the reader with a sense of beauty, or an insight into the human condition).

Based on these two additional criteria, it appears that there is an inverse correlation with loss. The lower the loss levels, the less 'legible' the generated text is, and with little to no poetic epiphanies that would strike the reader.

In the current model as seen in this submission - Checkpoint 100 - with a loss of 1.69, generated a stanza that drew three 'Likes' when I shared it on Facebook.

> *'Tis easy to love a little/
But hard to feel a little/
When you're dying -- so be it --/
I'll stay with you for life --/
I'll give it all I've got -- I think --/
And then I'm leaving --/
And no one will ever know/
That I ever loved you!*

However, in another early iteration that I had since accidentally deleted, but was generated around the first 200 checkpoints (when the loss is relatively higher), this suprisingly beautiful result (that was not only very legible, but also extremely high on the poetic epiphany scale) was created:

> *Beauty is the Seed/
Of life that does not sow./
It bears the Fertilizer/
And then -- it is gone.*

Incidentally, this poem generated a total of 15 'Likes' from my friends on Facebook - which is significantly better than the current model shown here!

Based on these observations, I would like to test this theory in future - to see if there is a particular loss rate that would be able to generate similarily good poetry that is balanced against 'legibility' and 'poetic epiphany'. 

In addition, I aim to continue experimenting with the recently released GPT-3 model, and see what new materials the model may accomplish.

Thank you for assessing my assignment!
"""